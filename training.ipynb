{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loss = 0.43 (approx)\n",
    "# training_acc = 0.8 (approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Hyperparameter Tuning\n",
    "# Voice Manipulations (Spectrogram)\n",
    "# Sort by Histogram (2 faces)\n",
    "# ResNeXt\n",
    "# EfficientNet\n",
    "# ResNet\n",
    "# XceptionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
    "from blazeface import BlazeFace\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.0-rc0\n",
      "OpenCV version: 4.1.2\n"
     ]
    }
   ],
   "source": [
    "# Versions\n",
    "\n",
    "print('Tensorflow version: 2.1.0-rc0')\n",
    "print('OpenCV version:', cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the paths\n",
    "\n",
    "input_path = '/kaggle/input/deepfake-detection-challenge/'\n",
    "train_dir = glob.glob(input_path + 'train_sample_videos/*.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the labels of training data\n",
    "\n",
    "df_train = pd.read_json(input_path + 'train_sample_videos/metadata.json').transpose()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f80e9ec4320>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the count of labels\n",
    "\n",
    "# Fake class is in majority\n",
    "df_train.label.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAKE    323\n",
       "REAL     77\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing video labels in a dictionary\n",
    "dic = {}\n",
    "for ind in df_train.index: \n",
    "    if(df_train['label'][ind] == 'REAL'):\n",
    "        dic[ind] = 0\n",
    "    else:\n",
    "        dic[ind] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the BlazeFace model weights\n",
    "\n",
    "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "facedet = BlazeFace().to(gpu)\n",
    "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
    "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
    "_ = facedet.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoReader:\n",
    "    \"\"\"Helper class for reading one or more frames from a video file.\"\"\"\n",
    "\n",
    "    def __init__(self, verbose=True, insets=(0, 0)):\n",
    "        \"\"\"Creates a new VideoReader.\n",
    "\n",
    "        Arguments:\n",
    "            verbose: whether to print warnings and error messages\n",
    "            insets: amount to inset the image by, as a percentage of \n",
    "                (width, height). This lets you \"zoom in\" to an image \n",
    "                to remove unimportant content around the borders. \n",
    "                Useful for face detection, which may not work if the \n",
    "                faces are too small.\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self.insets = insets\n",
    "\n",
    "    def read_frames(self, path, num_frames, jitter=0, seed=None):\n",
    "        \"\"\"Reads frames from 90th frame continuously.\n",
    "\n",
    "        Arguments:\n",
    "            path: the video file\n",
    "            num_frames: how many frames to read, -1 means the entire video\n",
    "                (warning: this will take up a lot of memory!)\n",
    "            jitter: if not 0, adds small random offsets to the frame indices;\n",
    "                this is useful so we don't always land on even or odd frames\n",
    "            seed: random seed for jittering; if you set this to a fixed value,\n",
    "                you probably want to set it only on the first video \n",
    "        \"\"\"\n",
    "        assert num_frames > 0\n",
    "\n",
    "        capture = cv2.VideoCapture(path)\n",
    "        # frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        # if frame_count <= 0: return None\n",
    "        frame_idxs = np.linspace(90, 90 + num_frames - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "        # frame_idxs = np.linspace(0, frame_count - 1, num_frames, endpoint=True, dtype=np.int)\n",
    "        if jitter > 0:\n",
    "            np.random.seed(seed)\n",
    "            jitter_offsets = np.random.randint(-jitter, jitter, len(frame_idxs))\n",
    "            frame_idxs = np.clip(frame_idxs + jitter_offsets, 0, frame_count - 1)\n",
    "\n",
    "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
    "        capture.release()\n",
    "        return result\n",
    "\n",
    "    def read_frames_at_indices(self, path, frame_idxs):\n",
    "        \"\"\"Reads frames from a video and puts them into a NumPy array.\n",
    "\n",
    "        Arguments:\n",
    "            path: the video file\n",
    "            frame_idxs: a list of frame indices. Important: should be\n",
    "                sorted from low-to-high! If an index appears multiple\n",
    "                times, the frame is still read only once.\n",
    "\n",
    "        Returns:\n",
    "            - a NumPy array of shape (num_frames, height, width, 3)\n",
    "            - a list of the frame indices that were read\n",
    "\n",
    "        Reading stops if loading a frame fails, in which case the first\n",
    "        dimension returned may actually be less than num_frames.\n",
    "\n",
    "        Returns None if an exception is thrown for any reason, or if no\n",
    "        frames were read.\n",
    "        \"\"\"\n",
    "        assert len(frame_idxs) > 0\n",
    "        capture = cv2.VideoCapture(path)\n",
    "        result = self._read_frames_at_indices(path, capture, frame_idxs)\n",
    "        capture.release()\n",
    "        return result\n",
    "\n",
    "    def _read_frames_at_indices(self, path, capture, frame_idxs):\n",
    "        try:\n",
    "            frames = []\n",
    "            idxs_read = []\n",
    "            for frame_idx in range(frame_idxs[0], frame_idxs[-1] + 1):\n",
    "                # Get the next frame, but don't decode if we're not using it.\n",
    "                ret = capture.grab()\n",
    "                if not ret:\n",
    "                    if self.verbose:\n",
    "                        print(\"Error grabbing frame %d from movie %s\" % (frame_idx, path))\n",
    "                    break\n",
    "\n",
    "                # Need to look at this frame?\n",
    "                current = len(idxs_read)\n",
    "                if frame_idx == frame_idxs[current]:\n",
    "                    ret, frame = capture.retrieve()\n",
    "                    if not ret or frame is None:\n",
    "                        if self.verbose:\n",
    "                            print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
    "                        break\n",
    "\n",
    "                    frame = self._postprocess_frame(frame)\n",
    "                    frames.append(frame)\n",
    "                    idxs_read.append(frame_idx)\n",
    "\n",
    "            if len(frames) > 0:\n",
    "                return np.stack(frames), idxs_read\n",
    "            if self.verbose:\n",
    "                print(\"No frames read from movie %s\" % path)\n",
    "            return None\n",
    "        except:\n",
    "            if self.verbose:\n",
    "                print(\"Exception while reading movie %s\" % path)\n",
    "            return None    \n",
    "\n",
    "    def _postprocess_frame(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.insets[0] > 0:\n",
    "            W = frame.shape[1]\n",
    "            p = int(W * self.insets[0])\n",
    "            frame = frame[:, p:-p, :]\n",
    "\n",
    "        if self.insets[1] > 0:\n",
    "            H = frame.shape[1]\n",
    "            q = int(H * self.insets[1])\n",
    "            frame = frame[q:-q, :, :]\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceExtractor:\n",
    "    \"\"\"Wrapper for face extraction workflow.\"\"\"\n",
    "    \n",
    "    def __init__(self, video_read_fn, facedetn):\n",
    "        \"\"\"Creates a new FaceExtractor.\n",
    "\n",
    "        Arguments:\n",
    "            video_read_fn: a function that takes in a path to a video file\n",
    "                and returns a tuple consisting of a NumPy array with shape\n",
    "                (num_frames, H, W, 3) and a list of frame indices, or None\n",
    "                in case of an error\n",
    "            facedet: the face detector object\n",
    "        \"\"\"\n",
    "        self.video_read_fn = video_read_fn\n",
    "        self.facedet = facedet\n",
    "    \n",
    "    def process_videos(self, input_dir, filenames, video_idxs):\n",
    "        \"\"\"For the specified selection of videos, grabs one or more frames \n",
    "        from each video, runs the face detector, and tries to find the faces \n",
    "        in each frame.\n",
    "\n",
    "        The frames are split into tiles, and the tiles from the different videos \n",
    "        are concatenated into a single batch. This means the face detector gets\n",
    "        a batch of size len(video_idxs) * num_frames * num_tiles (usually 3).\n",
    "\n",
    "        Arguments:\n",
    "            input_dir: base folder where the video files are stored\n",
    "            filenames: list of all video files in the input_dir\n",
    "            video_idxs: one or more indices from the filenames list; these\n",
    "                are the videos we'll actually process\n",
    "\n",
    "        Returns a list of dictionaries, one for each frame read from each video.\n",
    "\n",
    "        This dictionary contains:\n",
    "            - video_idx: the video this frame was taken from\n",
    "            - frame_idx: the index of the frame in the video\n",
    "            - frame_w, frame_h: original dimensions of the frame\n",
    "            - faces: a list containing zero or more NumPy arrays with a face crop\n",
    "            - scores: a list array with the confidence score for each face crop\n",
    "\n",
    "        If reading a video failed for some reason, it will not appear in the \n",
    "        output array. Note that there's no guarantee a given video will actually\n",
    "        have num_frames results (as soon as a reading problem is encountered for \n",
    "        a video, we continue with the next video).\n",
    "        \"\"\"\n",
    "        target_size = self.facedet.input_size\n",
    "\n",
    "        videos_read = []\n",
    "        frames_read = []\n",
    "        frames = []\n",
    "        tiles = []\n",
    "        resize_info = []\n",
    "        for video_idx in video_idxs:\n",
    "            # Read the full-size frames from this video.\n",
    "            filename = filenames[video_idx]\n",
    "            video_path = os.path.join(input_dir, filename)\n",
    "            result = self.video_read_fn(video_path)\n",
    "\n",
    "            # Error? Then skip this video.\n",
    "            if result is None: continue\n",
    "\n",
    "            videos_read.append(video_idx)\n",
    "\n",
    "            # Keep track of the original frames (need them later).\n",
    "            my_frames, my_idxs = result\n",
    "            frames.append(my_frames)\n",
    "            frames_read.append(my_idxs)\n",
    "\n",
    "            # Split the frames into several tiles. Resize the tiles to 128x128.\n",
    "            my_tiles, my_resize_info = self._tile_frames(my_frames, target_size)\n",
    "            tiles.append(my_tiles)\n",
    "            resize_info.append(my_resize_info)\n",
    "\n",
    "        # Put all the tiles for all the frames from all the videos into\n",
    "        # a single batch.\n",
    "        batch = np.concatenate(tiles)\n",
    "\n",
    "        # Run the face detector. The result is a list of PyTorch tensors, \n",
    "        # one for each image in the batch.\n",
    "        all_detections = self.facedet.predict_on_batch(batch, apply_nms=False)\n",
    "\n",
    "        result = []\n",
    "        offs = 0\n",
    "        for v in range(len(tiles)):\n",
    "            # Not all videos may have the same number of tiles, so find which \n",
    "            # detections go with which video.\n",
    "            num_tiles = tiles[v].shape[0]\n",
    "            detections = all_detections[offs:offs + num_tiles]\n",
    "            offs += num_tiles\n",
    "\n",
    "            # Convert the detections from 128x128 back to the original frame size.\n",
    "            detections = self._resize_detections(detections, target_size, resize_info[v])\n",
    "\n",
    "            # Because we have several tiles for each frame, combine the predictions\n",
    "            # from these tiles. The result is a list of PyTorch tensors, but now one\n",
    "            # for each frame (rather than each tile).\n",
    "            num_frames = frames[v].shape[0]\n",
    "            frame_size = (frames[v].shape[2], frames[v].shape[1])\n",
    "            detections = self._untile_detections(num_frames, frame_size, detections)\n",
    "\n",
    "            # The same face may have been detected in multiple tiles, so filter out\n",
    "            # overlapping detections. This is done separately for each frame.\n",
    "            detections = self.facedet.nms(detections)\n",
    "\n",
    "            for i in range(len(detections)):\n",
    "                # Crop the faces out of the original frame.\n",
    "                faces = self._add_margin_to_detections(detections[i], frame_size, 0.2)\n",
    "                faces = self._crop_faces(frames[v][i], faces)\n",
    "\n",
    "                # Add additional information about the frame and detections.\n",
    "                scores = list(detections[i][:, 16].cpu().numpy())\n",
    "                frame_dict = { \"video_idx\": videos_read[v],\n",
    "                               \"frame_idx\": frames_read[v][i],\n",
    "                               \"frame_w\": frame_size[0],\n",
    "                               \"frame_h\": frame_size[1],\n",
    "                               \"faces\": faces, \n",
    "                               \"scores\": scores }\n",
    "                result.append(frame_dict)\n",
    "\n",
    "                # TODO: could also add:\n",
    "                # - face rectangle in original frame coordinates\n",
    "                # - the keypoints (in crop coordinates)\n",
    "                \n",
    "        return result\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        \"\"\"Convenience method for doing face extraction on a single video.\"\"\"\n",
    "        input_dir = os.path.dirname(video_path)\n",
    "        filenames = [ os.path.basename(video_path) ]\n",
    "        return self.process_videos(input_dir, filenames, [0])\n",
    "\n",
    "    def _tile_frames(self, frames, target_size):\n",
    "        \"\"\"Splits each frame into several smaller, partially overlapping tiles\n",
    "        and resizes each tile to target_size.\n",
    "\n",
    "        After a bunch of experimentation, I found that for a 1920x1080 video,\n",
    "        BlazeFace works better on three 1080x1080 windows. These overlap by 420\n",
    "        pixels. (Two windows also work but it's best to have a clean center crop\n",
    "        in there as well.)\n",
    "\n",
    "        I also tried 6 windows of size 720x720 (horizontally: 720|360, 360|720;\n",
    "        vertically: 720|1200, 480|720|480, 1200|720) but that gives many false\n",
    "        positives when a window has no face in it.\n",
    "\n",
    "        For a video in portrait orientation (1080x1920), we only take a single\n",
    "        crop of the top-most 1080 pixels. If we split up the video vertically,\n",
    "        then we might get false positives again.\n",
    "\n",
    "        (NOTE: Not all videos are necessarily 1080p but the code can handle this.)\n",
    "\n",
    "        Arguments:\n",
    "            frames: NumPy array of shape (num_frames, height, width, 3)\n",
    "            target_size: (width, height)\n",
    "\n",
    "        Returns:\n",
    "            - a new (num_frames * N, target_size[1], target_size[0], 3) array\n",
    "              where N is the number of tiles used.\n",
    "            - a list [scale_w, scale_h, offset_x, offset_y] that describes how\n",
    "              to map the resized and cropped tiles back to the original image \n",
    "              coordinates. This is needed for scaling up the face detections \n",
    "              from the smaller image to the original image, so we can take the \n",
    "              face crops in the original coordinate space.    \n",
    "        \"\"\"\n",
    "        num_frames, H, W, _ = frames.shape\n",
    "\n",
    "        # Settings for 6 overlapping windows:\n",
    "        # split_size = 720\n",
    "        # x_step = 480\n",
    "        # y_step = 360\n",
    "        # num_v = 2\n",
    "        # num_h = 3\n",
    "\n",
    "        # Settings for 2 overlapping windows:\n",
    "        # split_size = min(H, W)\n",
    "        # x_step = W - split_size\n",
    "        # y_step = H - split_size\n",
    "        # num_v = 1\n",
    "        # num_h = 2 if W > H else 1\n",
    "\n",
    "        split_size = min(H, W)\n",
    "        x_step = (W - split_size) // 2\n",
    "        y_step = (H - split_size) // 2\n",
    "        num_v = 1\n",
    "        num_h = 3 if W > H else 1\n",
    "\n",
    "        splits = np.zeros((num_frames * num_v * num_h, target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "        i = 0\n",
    "        for f in range(num_frames):\n",
    "            y = 0\n",
    "            for v in range(num_v):\n",
    "                x = 0\n",
    "                for h in range(num_h):\n",
    "                    crop = frames[f, y:y+split_size, x:x+split_size, :]\n",
    "                    splits[i] = cv2.resize(crop, target_size, interpolation=cv2.INTER_AREA)\n",
    "                    x += x_step\n",
    "                    i += 1\n",
    "                y += y_step\n",
    "\n",
    "        resize_info = [split_size / target_size[0], split_size / target_size[1], 0, 0]\n",
    "        return splits, resize_info\n",
    "\n",
    "    def _resize_detections(self, detections, target_size, resize_info):\n",
    "        \"\"\"Converts a list of face detections back to the original \n",
    "        coordinate system.\n",
    "\n",
    "        Arguments:\n",
    "            detections: a list containing PyTorch tensors of shape (num_faces, 17) \n",
    "            target_size: (width, height)\n",
    "            resize_info: [scale_w, scale_h, offset_x, offset_y]\n",
    "        \"\"\"\n",
    "        projected = []\n",
    "        target_w, target_h = target_size\n",
    "        scale_w, scale_h, offset_x, offset_y = resize_info\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            detection = detections[i].clone()\n",
    "\n",
    "            # ymin, xmin, ymax, xmax\n",
    "            for k in range(2):\n",
    "                detection[:, k*2    ] = (detection[:, k*2    ] * target_h - offset_y) * scale_h\n",
    "                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_w - offset_x) * scale_w\n",
    "\n",
    "            # keypoints are x,y\n",
    "            for k in range(2, 8):\n",
    "                detection[:, k*2    ] = (detection[:, k*2    ] * target_w - offset_x) * scale_w\n",
    "                detection[:, k*2 + 1] = (detection[:, k*2 + 1] * target_h - offset_y) * scale_h\n",
    "\n",
    "            projected.append(detection)\n",
    "\n",
    "        return projected    \n",
    "    \n",
    "    def _untile_detections(self, num_frames, frame_size, detections):\n",
    "        \"\"\"With N tiles per frame, there also are N times as many detections.\n",
    "        This function groups together the detections for a given frame; it is\n",
    "        the complement to tile_frames().\n",
    "        \"\"\"\n",
    "        combined_detections = []\n",
    "\n",
    "        W, H = frame_size\n",
    "        split_size = min(H, W)\n",
    "        x_step = (W - split_size) // 2\n",
    "        y_step = (H - split_size) // 2\n",
    "        num_v = 1\n",
    "        num_h = 3 if W > H else 1\n",
    "\n",
    "        i = 0\n",
    "        for f in range(num_frames):\n",
    "            detections_for_frame = []\n",
    "            y = 0\n",
    "            for v in range(num_v):\n",
    "                x = 0\n",
    "                for h in range(num_h):\n",
    "                    # Adjust the coordinates based on the split positions.\n",
    "                    detection = detections[i].clone()\n",
    "                    if detection.shape[0] > 0:\n",
    "                        for k in range(2):\n",
    "                            detection[:, k*2    ] += y\n",
    "                            detection[:, k*2 + 1] += x\n",
    "                        for k in range(2, 8):\n",
    "                            detection[:, k*2    ] += x\n",
    "                            detection[:, k*2 + 1] += y\n",
    "\n",
    "                    detections_for_frame.append(detection)\n",
    "                    x += x_step\n",
    "                    i += 1\n",
    "                y += y_step\n",
    "\n",
    "            combined_detections.append(torch.cat(detections_for_frame))\n",
    "\n",
    "        return combined_detections\n",
    "    \n",
    "    def _add_margin_to_detections(self, detections, frame_size, margin=0.2):\n",
    "        \"\"\"Expands the face bounding box.\n",
    "\n",
    "        NOTE: The face detections often do not include the forehead, which\n",
    "        is why we use twice the margin for ymin.\n",
    "\n",
    "        Arguments:\n",
    "            detections: a PyTorch tensor of shape (num_detections, 17)\n",
    "            frame_size: maximum (width, height)\n",
    "            margin: a percentage of the bounding box's height\n",
    "\n",
    "        Returns a PyTorch tensor of shape (num_detections, 17).\n",
    "        \"\"\"\n",
    "        offset = torch.round(margin * (detections[:, 2] - detections[:, 0]))\n",
    "        detections = detections.clone()\n",
    "        detections[:, 0] = torch.clamp(detections[:, 0] - offset*2, min=0)            # ymin\n",
    "        detections[:, 1] = torch.clamp(detections[:, 1] - offset, min=0)              # xmin\n",
    "        detections[:, 2] = torch.clamp(detections[:, 2] + offset, max=frame_size[1])  # ymax\n",
    "        detections[:, 3] = torch.clamp(detections[:, 3] + offset, max=frame_size[0])  # xmax\n",
    "        return detections\n",
    "    \n",
    "    def _crop_faces(self, frame, detections):\n",
    "        \"\"\"Copies the face region(s) from the given frame into a set\n",
    "        of new NumPy arrays.\n",
    "\n",
    "        Arguments:\n",
    "            frame: a NumPy array of shape (H, W, 3)\n",
    "            detections: a PyTorch tensor of shape (num_detections, 17)\n",
    "\n",
    "        Returns a list of NumPy arrays, one for each face crop. If there\n",
    "        are no faces detected for this frame, returns an empty list.\n",
    "        \"\"\"\n",
    "        faces = []\n",
    "        for i in range(len(detections)):\n",
    "            ymin, xmin, ymax, xmax = detections[i, :4].cpu().numpy().astype(np.int)\n",
    "            face = frame[ymin:ymax, xmin:xmax, :]\n",
    "            faces.append(face)\n",
    "        return faces\n",
    "\n",
    "    def keep_only_best_face(self, crops):\n",
    "        \"\"\"For each frame, only keeps the face with the highest confidence. \n",
    "        \n",
    "        This gets rid of false positives, but obviously is problematic for \n",
    "        videos with two people!\n",
    "\n",
    "        This is an optional postprocessing step. Modifies the original\n",
    "        data structure.\n",
    "        \"\"\"\n",
    "        for i in range(len(crops)):\n",
    "            frame_data = crops[i]\n",
    "            if len(frame_data[\"faces\"]) > 0:\n",
    "                frame_data[\"faces\"] = frame_data[\"faces\"][:1]\n",
    "                frame_data[\"scores\"] = frame_data[\"scores\"][:1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
    "    h, w = img.shape[:2]\n",
    "    if w > h:\n",
    "        h = h * size // w\n",
    "        w = size\n",
    "    else:\n",
    "        w = w * size // h\n",
    "        h = size\n",
    "\n",
    "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def make_square_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "    size = max(h, w)\n",
    "    t = 0\n",
    "    b = size - h\n",
    "    l = 0\n",
    "    r = size - w\n",
    "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_extract_on_video(video_path):\n",
    "    # Find the faces for frames_per_video captured frames in the video\n",
    "    faces = face_extractor.process_video(video_path)\n",
    "\n",
    "    # In case only one person in the video, the other detections are false positives\n",
    "    # The face with highest confidence is taken\n",
    "    face_extractor.keep_only_best_face(faces)\n",
    "    # TODO: def sort_by_histogram(self, crops) for videos with 2 people\n",
    "\n",
    "    features = []\n",
    "    for frame_data in faces:\n",
    "        for face in frame_data[\"faces\"]:\n",
    "            face = make_square_image(isotropically_resize_image(face, input_size))\n",
    "\n",
    "            # BGR to RGB\n",
    "            features.append(np.asarray(face)[:,:,::-1])\n",
    "\n",

    "    x = frames_per_video - np.array(features).shape[0]\n",
    "\n",
    "    # No face found\n",
    "    if(x == frames_per_video):\n",
    "        return(None, None)\n",
    "\n",
    "    if(x != 0):\n",
    "        # Pre-padding for efficient training (Done when features found is less than frames_per_video)\n",
    "        features = np.concatenate((np.zeros((x, 229, 229, 3)), np.array(features)))\n",
    "\n",
    "    # Preprocessing specific to Keras implementation of Inception-V3\n",
    "    features = preprocess_input(np.array(features))\n",
    "    return(features, dic[video_path.split('/')[-1]])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_extract_on_video_set(video_paths):\n",
    "    # Find the faces (ROI) for frames_per_video frames in the the given videos\n",
    "    roi = []\n",
    "    labels = []\n",
    "    for x in video_paths:\n",
    "        features, label = face_extract_on_video(x)\n",
    "        if(label == None):\n",
    "            continue\n",
    "        roi.append(features)\n",
    "        labels.append(label)\n",
    "\n",
    "        # Oversampling\n",
    "        if(label == 0):\n",
    "            roi.append(features)\n",
    "            roi.append(features)\n",
    "            labels.append(label)\n",
    "            labels.append(label)\n",
    "    return(roi, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video = 20  # 20 frames starting from 90th frame will be captured\n",
    "input_size = 229       # 229x229x3 (square-fitted)\n",
    "batch_size = 64\n",
    "# epochs = 160\n",
    "n = len(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 229, 229, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 114, 114, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 114, 114, 32) 96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 114, 114, 32) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 32) 9216        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 112, 112, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 112, 112, 64) 18432       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 112, 112, 64) 192         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 112, 112, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 55, 55, 64)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 55, 55, 80)   5120        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 55, 55, 80)   240         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 55, 55, 80)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 53, 53, 192)  138240      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 53, 53, 192)  576         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 53, 53, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 26, 26, 192)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 26, 26, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 26, 26, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 26, 26, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 26, 26, 48)   9216        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 26, 26, 96)   55296       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 26, 26, 48)   144         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 26, 26, 96)   288         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 26, 26, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 26, 26, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 26, 26, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 26, 26, 64)   12288       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 26, 26, 64)   76800       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 26, 26, 96)   82944       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 26, 26, 32)   6144        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 26, 26, 64)   192         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 26, 26, 64)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 26, 26, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 26, 26, 32)   96          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 26, 26, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 26, 26, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 26, 26, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 26, 26, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 26, 26, 256)  0           activation_5[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 26, 26, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 26, 26, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 26, 26, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 26, 26, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 26, 26, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 26, 26, 48)   144         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 26, 26, 96)   288         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 26, 26, 48)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 26, 26, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 26, 26, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 26, 26, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 26, 26, 64)   76800       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 26, 26, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 26, 26, 64)   16384       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 26, 26, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 26, 26, 64)   192         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 26, 26, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 26, 26, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 26, 26, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 26, 26, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 26, 26, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 26, 26, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 26, 26, 288)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 26, 26, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 26, 26, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 26, 26, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 26, 26, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 26, 26, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 26, 26, 48)   144         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 26, 26, 96)   288         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 26, 26, 48)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 26, 26, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 26, 26, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 26, 26, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 26, 26, 64)   76800       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 26, 26, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 26, 26, 64)   18432       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 26, 26, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 26, 26, 64)   192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 26, 26, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 26, 26, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 26, 26, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 26, 26, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 26, 26, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 26, 26, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 26, 26, 288)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 26, 26, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 26, 26, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 26, 26, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 26, 26, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 26, 26, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 26, 26, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 12, 12, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 12, 12, 96)   82944       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 12, 12, 384)  1152        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 12, 12, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 12, 12, 384)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 12, 12, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 12, 12, 768)  0           activation_26[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 12, 12, 128)  384         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 12, 12, 128)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 12, 12, 128)  114688      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 12, 12, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 12, 12, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 12, 12, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 12, 12, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 12, 12, 128)  384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 12, 12, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 12, 12, 128)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 12, 12, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 12, 12, 128)  114688      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 12, 12, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 12, 12, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 12, 12, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 12, 12, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 12, 12, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 12, 12, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 12, 12, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 12, 12, 192)  172032      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 12, 12, 192)  172032      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 12, 12, 192)  576         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 12, 12, 192)  576         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 12, 12, 192)  576         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 12, 12, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 12, 12, 192)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 12, 12, 192)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 12, 12, 192)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 12, 12, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 12, 12, 768)  0           activation_30[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 12, 12, 160)  480         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 12, 12, 160)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 12, 12, 160)  179200      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 12, 12, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 12, 12, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 12, 12, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 12, 12, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 12, 12, 160)  480         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 12, 12, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 12, 12, 160)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 12, 12, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 12, 12, 160)  179200      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 12, 12, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 12, 12, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 12, 12, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 12, 12, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 12, 12, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 12, 12, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 12, 12, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 12, 12, 192)  215040      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 12, 12, 192)  215040      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 12, 12, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 12, 12, 192)  576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 12, 12, 192)  576         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 12, 12, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 12, 12, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 12, 12, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 12, 12, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 12, 12, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 12, 12, 768)  0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 activation_48[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 12, 12, 160)  480         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 12, 12, 160)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 12, 12, 160)  179200      activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 12, 12, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 12, 12, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 12, 12, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 12, 12, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 12, 12, 160)  480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 12, 12, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 12, 12, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 12, 12, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 12, 12, 160)  179200      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 12, 12, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 12, 12, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 12, 12, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 12, 12, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 12, 12, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 12, 12, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 12, 12, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 12, 12, 192)  215040      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 12, 12, 192)  215040      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 12, 12, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 12, 12, 192)  576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 12, 12, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 12, 12, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 12, 12, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 12, 12, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 12, 12, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 12, 12, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 12, 12, 768)  0           activation_50[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "                                                                 activation_58[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 12, 12, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 12, 12, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 12, 12, 192)  258048      activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 12, 12, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 12, 12, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 12, 12, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 12, 12, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 12, 12, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 12, 12, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 12, 12, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 12, 12, 192)  258048      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 12, 12, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 12, 12, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 12, 12, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 12, 12, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 12, 12, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 12, 12, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 12, 12, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 12, 12, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 12, 12, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 12, 12, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 12, 12, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 12, 12, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 12, 12, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 12, 12, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 12, 12, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 12, 12, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 12, 12, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 12, 12, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 12, 12, 768)  0           activation_60[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "                                                                 activation_68[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 12, 12, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 12, 12, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 12, 12, 192)  258048      activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 12, 12, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 12, 12, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 12, 12, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 12, 12, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 12, 12, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 12, 12, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 12, 12, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 12, 12, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 5, 5, 320)    552960      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 5, 5, 192)    331776      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 5, 5, 320)    960         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 5, 5, 192)    576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 5, 5, 320)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 5, 5, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 5, 5, 1280)   0           activation_71[0][0]              \n",
      "                                                                 activation_75[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 5, 5, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 5, 5, 448)    1344        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 5, 5, 448)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 5, 5, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 5, 5, 384)    1548288     activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 5, 5, 384)    1152        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 5, 5, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 5, 5, 384)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 5, 5, 384)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 5, 5, 384)    442368      activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 5, 5, 384)    442368      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 5, 5, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 5, 5, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 5, 5, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 5, 5, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 5, 5, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 5, 5, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 5, 5, 192)    245760      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 5, 5, 320)    960         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 5, 5, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 5, 5, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 5, 5, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 5, 5, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 5, 5, 192)    576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 5, 5, 320)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 5, 5, 768)    0           activation_78[0][0]              \n",
      "                                                                 activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 5, 5, 768)    0           activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 5, 5, 192)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 5, 5, 2048)   0           activation_76[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 5, 5, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 5, 5, 448)    1344        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 5, 5, 448)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 5, 5, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 5, 5, 384)    1548288     activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 5, 5, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 5, 5, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 5, 5, 384)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 5, 5, 384)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 5, 5, 384)    442368      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 5, 5, 384)    442368      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 5, 5, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 5, 5, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 5, 5, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 5, 5, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 5, 5, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 5, 5, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 5, 5, 192)    393216      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 5, 5, 320)    960         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 5, 5, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 5, 5, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 5, 5, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 5, 5, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 5, 5, 192)    576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 5, 5, 320)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 5, 5, 768)    0           activation_87[0][0]              \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 5, 768)    0           activation_91[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 5, 5, 192)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 5, 5, 2048)   0           activation_85[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           mixed10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Taking the base model as Inception V3 and initializing its weight with imagenet\n",
    "\n",
    "input_tensor = Input(shape = (229, 229, 3))\n",
    "cnn_model = InceptionV3(input_tensor = input_tensor, weights = None, include_top = False, pooling = 'avg')\n",
    "cnn_model.load_weights('/kaggle/input/inception-pretrained/inception_v3.h5')\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               2360320   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 2,426,626\n",
      "Trainable params: 2,426,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model architecture\n",
    "\n",
    "rnn = Sequential()\n",
    "\n",
    "# LSTM layer\n",
    "rnn.add(LSTM(256, return_sequences = False, dropout = 0.5, recurrent_dropout = 0.5, input_shape = (frames_per_video, 2048)))\n",
    "\n",
    "# Fully Connected Layer\n",
    "rnn.add(Dense(256, activation = 'tanh'))\n",
    "\n",
    "# Dropout for regularization\n",
    "rnn.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "rnn.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_reader = VideoReader()\n",
    "video_read_fn = lambda x: video_reader.read_frames(x, num_frames = frames_per_video)\n",
    "face_extractor = FaceExtractor(video_read_fn, facedet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI extraction time taken:  460.07745456695557\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "roi, labels = face_extract_on_video_set(train_dir)\n",
    "print('ROI extraction time taken: ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding train_labels\n",
    "train_labels = []\n",
    "for x in labels:\n",
    "    if(x == 0):\n",
    "        train_labels.append([1, 0])\n",
    "    else:\n",
    "        train_labels.append([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530, 20, 229, 229, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(roi).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction time taken:  532.0822386741638\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "result = []\n",
    "for i in range(np.array(roi).shape[0]):\n",
    "    result.append(cnn_model.predict(np.array(roi[i])))\n",
    "print('Feature extraction time taken: ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530, 20, 2048)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(result).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(530, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_acc():\n",
    "    while(True):\n",
    "        t1 = time.time()\n",
    "        history = rnn.fit(np.array(result), np.array(train_labels), epochs = 1, batch_size = batch_size, shuffle = False, verbose = 0)\n",
    "        print('Epoch: {}' .format(count))\n",
    "        print('{}/{} [==============================] - {}s - loss: {} - accuracy: {}' .format(x, x, '%.2f'%(time.time() - t1), '%.4f'%(history.history['loss'][-1]), '%.4f'%(history.history['accuracy'][-1])))\n",
    "        count += 1\n",
    "        if(history.history['accuracy'][-1] >= 0.8):\n",
    "            print('Model saved based on threshold accuracy...')\n",
    "            rnn.save('deepfake_predictor_acc_0.8.h5')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop_loss(count):\n",
    "    while(True):\n",
    "        t1 = time.time()\n",
    "        history = rnn.fit(np.array(result), np.array(train_labels), epochs = 1, batch_size = batch_size, shuffle = False, verbose = 0)\n",
    "        print('Epoch: {}' .format(count))\n",
    "        print('{}/{} [==============================] - {}s - loss: {} - accuracy: {}' .format(x, x, '%.2f'%(time.time() - t1), '%.4f'%(history.history['loss'][-1]), '%.4f'%(history.history['accuracy'][-1])))\n",
    "        count += 1\n",
    "        if(history.history['loss'][-1] <= 0.43):\n",
    "            print('Model saved based on threshold loss...')\n",
    "            rnn.save('deepfake_predictor_loss_0.43.h5')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 530 samples...\n",
      "Epoch: 1\n",
      "530/530 [==============================] - 5.39s - loss: 0.8332 - accuracy: 0.5415\n",
      "Epoch: 2\n",
      "530/530 [==============================] - 3.15s - loss: 0.8135 - accuracy: 0.4962\n",
      "Epoch: 3\n",
      "530/530 [==============================] - 2.95s - loss: 0.7829 - accuracy: 0.4755\n",
      "Epoch: 4\n",
      "530/530 [==============================] - 2.94s - loss: 0.7748 - accuracy: 0.5151\n",
      "Epoch: 5\n",
      "530/530 [==============================] - 2.97s - loss: 0.7024 - accuracy: 0.5642\n",
      "Epoch: 6\n",
      "530/530 [==============================] - 2.97s - loss: 0.7376 - accuracy: 0.5321\n",
      "Epoch: 7\n",
      "530/530 [==============================] - 2.95s - loss: 0.7005 - accuracy: 0.5698\n",
      "Epoch: 8\n",
      "530/530 [==============================] - 2.96s - loss: 0.6974 - accuracy: 0.5547\n",
      "Epoch: 9\n",
      "530/530 [==============================] - 2.99s - loss: 0.7032 - accuracy: 0.5736\n",
      "Epoch: 10\n",
      "530/530 [==============================] - 3.00s - loss: 0.6869 - accuracy: 0.5698\n",
      "Epoch: 11\n",
      "530/530 [==============================] - 2.97s - loss: 0.6744 - accuracy: 0.5962\n",
      "Epoch: 12\n",
      "530/530 [==============================] - 2.95s - loss: 0.6850 - accuracy: 0.5679\n",
      "Epoch: 13\n",
      "530/530 [==============================] - 2.98s - loss: 0.6917 - accuracy: 0.5698\n",
      "Epoch: 14\n",
      "530/530 [==============================] - 2.98s - loss: 0.6961 - accuracy: 0.5226\n",
      "Epoch: 15\n",
      "530/530 [==============================] - 2.95s - loss: 0.7015 - accuracy: 0.5491\n",
      "Epoch: 16\n",
      "530/530 [==============================] - 2.91s - loss: 0.6828 - accuracy: 0.5943\n",
      "Epoch: 17\n",
      "530/530 [==============================] - 2.97s - loss: 0.6875 - accuracy: 0.5566\n",
      "Epoch: 18\n",
      "530/530 [==============================] - 2.91s - loss: 0.6646 - accuracy: 0.5887\n",
      "Epoch: 19\n",
      "530/530 [==============================] - 2.89s - loss: 0.6812 - accuracy: 0.5604\n",
      "Epoch: 20\n",
      "530/530 [==============================] - 3.16s - loss: 0.6851 - accuracy: 0.5943\n",
      "Epoch: 21\n",
      "530/530 [==============================] - 3.02s - loss: 0.6805 - accuracy: 0.5698\n",
      "Epoch: 22\n",
      "530/530 [==============================] - 3.24s - loss: 0.6635 - accuracy: 0.5792\n",
      "Epoch: 23\n",
      "530/530 [==============================] - 2.93s - loss: 0.6553 - accuracy: 0.6245\n",
      "Epoch: 24\n",
      "530/530 [==============================] - 3.11s - loss: 0.6898 - accuracy: 0.5566\n",
      "Epoch: 25\n",
      "530/530 [==============================] - 2.92s - loss: 0.6669 - accuracy: 0.5660\n",
      "Epoch: 26\n",
      "530/530 [==============================] - 2.97s - loss: 0.6417 - accuracy: 0.6000\n",
      "Epoch: 27\n",
      "530/530 [==============================] - 2.86s - loss: 0.6472 - accuracy: 0.6113\n",
      "Epoch: 28\n",
      "530/530 [==============================] - 3.05s - loss: 0.6478 - accuracy: 0.6226\n",
      "Epoch: 29\n",
      "530/530 [==============================] - 2.99s - loss: 0.6421 - accuracy: 0.6000\n",
      "Epoch: 30\n",
      "530/530 [==============================] - 2.93s - loss: 0.6347 - accuracy: 0.6208\n",
      "Epoch: 31\n",
      "530/530 [==============================] - 2.96s - loss: 0.6209 - accuracy: 0.6264\n",
      "Epoch: 32\n",
      "530/530 [==============================] - 3.01s - loss: 0.6512 - accuracy: 0.5736\n",
      "Epoch: 33\n",
      "530/530 [==============================] - 2.93s - loss: 0.6355 - accuracy: 0.6094\n",
      "Epoch: 34\n",
      "530/530 [==============================] - 2.93s - loss: 0.6098 - accuracy: 0.6472\n",
      "Epoch: 35\n",
      "530/530 [==============================] - 2.96s - loss: 0.6221 - accuracy: 0.6283\n",
      "Epoch: 36\n",
      "530/530 [==============================] - 3.00s - loss: 0.6569 - accuracy: 0.5925\n",
      "Epoch: 37\n",
      "530/530 [==============================] - 2.95s - loss: 0.6564 - accuracy: 0.5566\n",
      "Epoch: 38\n",
      "530/530 [==============================] - 2.99s - loss: 0.6781 - accuracy: 0.5472\n",
      "Epoch: 39\n",
      "530/530 [==============================] - 2.93s - loss: 0.6195 - accuracy: 0.6094\n",
      "Epoch: 40\n",
      "530/530 [==============================] - 3.25s - loss: 0.6314 - accuracy: 0.6000\n",
      "Epoch: 41\n",
      "530/530 [==============================] - 3.21s - loss: 0.6268 - accuracy: 0.6019\n",
      "Epoch: 42\n",
      "530/530 [==============================] - 2.92s - loss: 0.6572 - accuracy: 0.6094\n",
      "Epoch: 43\n",
      "530/530 [==============================] - 3.02s - loss: 0.6346 - accuracy: 0.5962\n",
      "Epoch: 44\n",
      "530/530 [==============================] - 2.95s - loss: 0.6593 - accuracy: 0.5566\n",
      "Epoch: 45\n",
      "530/530 [==============================] - 2.93s - loss: 0.6354 - accuracy: 0.5774\n",
      "Epoch: 46\n",
      "530/530 [==============================] - 2.93s - loss: 0.6402 - accuracy: 0.6038\n",
      "Epoch: 47\n",
      "530/530 [==============================] - 2.93s - loss: 0.6202 - accuracy: 0.6057\n",
      "Epoch: 48\n",
      "530/530 [==============================] - 2.88s - loss: 0.6251 - accuracy: 0.6528\n",
      "Epoch: 49\n",
      "530/530 [==============================] - 2.98s - loss: 0.6272 - accuracy: 0.6453\n",
      "Epoch: 50\n",
      "530/530 [==============================] - 2.99s - loss: 0.6380 - accuracy: 0.6170\n",
      "Epoch: 51\n",
      "530/530 [==============================] - 2.94s - loss: 0.6084 - accuracy: 0.6170\n",
      "Epoch: 52\n",
      "530/530 [==============================] - 2.98s - loss: 0.6101 - accuracy: 0.6170\n",
      "Epoch: 53\n",
      "530/530 [==============================] - 2.95s - loss: 0.6161 - accuracy: 0.6245\n",
      "Epoch: 54\n",
      "530/530 [==============================] - 2.99s - loss: 0.6068 - accuracy: 0.6283\n",
      "Epoch: 55\n",
      "530/530 [==============================] - 3.04s - loss: 0.6189 - accuracy: 0.6094\n",
      "Epoch: 56\n",
      "530/530 [==============================] - 2.98s - loss: 0.6116 - accuracy: 0.6509\n",
      "Epoch: 57\n",
      "530/530 [==============================] - 3.05s - loss: 0.5941 - accuracy: 0.6604\n",
      "Epoch: 58\n",
      "530/530 [==============================] - 2.92s - loss: 0.6060 - accuracy: 0.6434\n",
      "Epoch: 59\n",
      "530/530 [==============================] - 2.98s - loss: 0.6058 - accuracy: 0.6396\n",
      "Epoch: 60\n",
      "530/530 [==============================] - 3.15s - loss: 0.6196 - accuracy: 0.6283\n",
      "Epoch: 61\n",
      "530/530 [==============================] - 3.10s - loss: 0.6272 - accuracy: 0.6377\n",
      "Epoch: 62\n",
      "530/530 [==============================] - 3.23s - loss: 0.6188 - accuracy: 0.6472\n",
      "Epoch: 63\n",
      "530/530 [==============================] - 2.99s - loss: 0.5889 - accuracy: 0.6755\n",
      "Epoch: 64\n",
      "530/530 [==============================] - 3.01s - loss: 0.6072 - accuracy: 0.6453\n",
      "Epoch: 65\n",
      "530/530 [==============================] - 2.99s - loss: 0.6210 - accuracy: 0.6415\n",
      "Epoch: 66\n",
      "530/530 [==============================] - 3.00s - loss: 0.6004 - accuracy: 0.6434\n",
      "Epoch: 67\n",
      "530/530 [==============================] - 2.92s - loss: 0.6014 - accuracy: 0.6472\n",
      "Epoch: 68\n",
      "530/530 [==============================] - 2.98s - loss: 0.5932 - accuracy: 0.6830\n",
      "Epoch: 69\n",
      "530/530 [==============================] - 2.98s - loss: 0.5660 - accuracy: 0.7000\n",
      "Epoch: 70\n",
      "530/530 [==============================] - 2.98s - loss: 0.5872 - accuracy: 0.6604\n",
      "Epoch: 71\n",
      "530/530 [==============================] - 2.90s - loss: 0.6021 - accuracy: 0.6792\n",
      "Epoch: 72\n",
      "530/530 [==============================] - 2.99s - loss: 0.5851 - accuracy: 0.6604\n",
      "Epoch: 73\n",
      "530/530 [==============================] - 3.00s - loss: 0.5830 - accuracy: 0.6906\n",
      "Epoch: 74\n",
      "530/530 [==============================] - 2.90s - loss: 0.5639 - accuracy: 0.6717\n",
      "Epoch: 75\n",
      "530/530 [==============================] - 3.04s - loss: 0.5993 - accuracy: 0.6321\n",
      "Epoch: 76\n",
      "530/530 [==============================] - 2.98s - loss: 0.6147 - accuracy: 0.5925\n",
      "Epoch: 77\n",
      "530/530 [==============================] - 2.95s - loss: 0.6052 - accuracy: 0.6075\n",
      "Epoch: 78\n",
      "530/530 [==============================] - 2.94s - loss: 0.5951 - accuracy: 0.6434\n",
      "Epoch: 79\n",
      "530/530 [==============================] - 2.95s - loss: 0.5981 - accuracy: 0.6302\n",
      "Epoch: 80\n",
      "530/530 [==============================] - 3.15s - loss: 0.5743 - accuracy: 0.6868\n",
      "Epoch: 81\n",
      "530/530 [==============================] - 3.13s - loss: 0.6190 - accuracy: 0.6377\n",
      "Epoch: 82\n",
      "530/530 [==============================] - 2.94s - loss: 0.5688 - accuracy: 0.6792\n",
      "Epoch: 83\n",
      "530/530 [==============================] - 3.17s - loss: 0.5908 - accuracy: 0.6604\n",
      "Epoch: 84\n",
      "530/530 [==============================] - 2.99s - loss: 0.5794 - accuracy: 0.6566\n",
      "Epoch: 85\n",
      "530/530 [==============================] - 2.93s - loss: 0.6067 - accuracy: 0.6377\n",
      "Epoch: 86\n",
      "530/530 [==============================] - 2.95s - loss: 0.5897 - accuracy: 0.6434\n",
      "Epoch: 87\n",
      "530/530 [==============================] - 2.94s - loss: 0.6012 - accuracy: 0.6113\n",
      "Epoch: 88\n",
      "530/530 [==============================] - 2.97s - loss: 0.5955 - accuracy: 0.6208\n",
      "Epoch: 89\n",
      "530/530 [==============================] - 2.93s - loss: 0.5870 - accuracy: 0.6396\n",
      "Epoch: 90\n",
      "530/530 [==============================] - 3.04s - loss: 0.5834 - accuracy: 0.6453\n",
      "Epoch: 91\n",
      "530/530 [==============================] - 2.98s - loss: 0.5917 - accuracy: 0.6604\n",
      "Epoch: 92\n",
      "530/530 [==============================] - 3.03s - loss: 0.5863 - accuracy: 0.6906\n",
      "Epoch: 93\n",
      "530/530 [==============================] - 2.96s - loss: 0.5794 - accuracy: 0.6736\n",
      "Epoch: 94\n",
      "530/530 [==============================] - 2.99s - loss: 0.5986 - accuracy: 0.6547\n",
      "Epoch: 95\n",
      "530/530 [==============================] - 2.91s - loss: 0.5644 - accuracy: 0.6943\n",
      "Epoch: 96\n",
      "530/530 [==============================] - 2.83s - loss: 0.5905 - accuracy: 0.6660\n",
      "Epoch: 97\n",
      "530/530 [==============================] - 2.97s - loss: 0.5791 - accuracy: 0.6642\n",
      "Epoch: 98\n",
      "530/530 [==============================] - 3.09s - loss: 0.6129 - accuracy: 0.6415\n",
      "Epoch: 99\n",
      "530/530 [==============================] - 2.87s - loss: 0.5869 - accuracy: 0.6566\n",
      "Epoch: 100\n",
      "530/530 [==============================] - 3.03s - loss: 0.5956 - accuracy: 0.6642\n",
      "Epoch: 101\n",
      "530/530 [==============================] - 2.97s - loss: 0.5809 - accuracy: 0.6792\n",
      "Epoch: 102\n",
      "530/530 [==============================] - 3.12s - loss: 0.5997 - accuracy: 0.6245\n",
      "Epoch: 103\n",
      "530/530 [==============================] - 2.91s - loss: 0.5748 - accuracy: 0.6698\n",
      "Epoch: 104\n",
      "530/530 [==============================] - 3.05s - loss: 0.5831 - accuracy: 0.6717\n",
      "Epoch: 105\n",
      "530/530 [==============================] - 2.95s - loss: 0.5859 - accuracy: 0.6830\n",
      "Epoch: 106\n",
      "530/530 [==============================] - 2.91s - loss: 0.5797 - accuracy: 0.6698\n",
      "Epoch: 107\n",
      "530/530 [==============================] - 2.86s - loss: 0.5853 - accuracy: 0.6377\n",
      "Epoch: 108\n",
      "530/530 [==============================] - 2.95s - loss: 0.5746 - accuracy: 0.6792\n",
      "Epoch: 109\n",
      "530/530 [==============================] - 3.02s - loss: 0.5652 - accuracy: 0.7132\n",
      "Epoch: 110\n",
      "530/530 [==============================] - 2.88s - loss: 0.5736 - accuracy: 0.6623\n",
      "Epoch: 111\n",
      "530/530 [==============================] - 2.89s - loss: 0.5379 - accuracy: 0.7151\n",
      "Epoch: 112\n",
      "530/530 [==============================] - 2.88s - loss: 0.5588 - accuracy: 0.7000\n",
      "Epoch: 113\n",
      "530/530 [==============================] - 2.94s - loss: 0.5603 - accuracy: 0.6736\n",
      "Epoch: 114\n",
      "530/530 [==============================] - 2.96s - loss: 0.5765 - accuracy: 0.6377\n",
      "Epoch: 115\n",
      "530/530 [==============================] - 2.96s - loss: 0.5807 - accuracy: 0.6679\n",
      "Epoch: 116\n",
      "530/530 [==============================] - 2.84s - loss: 0.5564 - accuracy: 0.6849\n",
      "Epoch: 117\n",
      "530/530 [==============================] - 2.90s - loss: 0.5659 - accuracy: 0.6717\n",
      "Epoch: 118\n",
      "530/530 [==============================] - 2.92s - loss: 0.5686 - accuracy: 0.6717\n",
      "Epoch: 119\n",
      "530/530 [==============================] - 2.98s - loss: 0.5605 - accuracy: 0.6849\n",
      "Epoch: 120\n",
      "530/530 [==============================] - 3.03s - loss: 0.5921 - accuracy: 0.6811\n",
      "Epoch: 121\n",
      "530/530 [==============================] - 3.26s - loss: 0.5626 - accuracy: 0.6906\n",
      "Epoch: 122\n",
      "530/530 [==============================] - 3.15s - loss: 0.5725 - accuracy: 0.6943\n",
      "Epoch: 123\n",
      "530/530 [==============================] - 2.95s - loss: 0.5705 - accuracy: 0.6906\n",
      "Epoch: 124\n",
      "530/530 [==============================] - 3.03s - loss: 0.5463 - accuracy: 0.6792\n",
      "Epoch: 125\n",
      "530/530 [==============================] - 2.88s - loss: 0.6091 - accuracy: 0.6547\n",
      "Epoch: 126\n",
      "530/530 [==============================] - 2.94s - loss: 0.5670 - accuracy: 0.6604\n",
      "Epoch: 127\n",
      "530/530 [==============================] - 2.92s - loss: 0.5609 - accuracy: 0.6736\n",
      "Epoch: 128\n",
      "530/530 [==============================] - 3.05s - loss: 0.5500 - accuracy: 0.7000\n",
      "Epoch: 129\n",
      "530/530 [==============================] - 2.93s - loss: 0.5720 - accuracy: 0.7075\n",
      "Epoch: 130\n",
      "530/530 [==============================] - 2.97s - loss: 0.5610 - accuracy: 0.6774\n",
      "Epoch: 131\n",
      "530/530 [==============================] - 2.93s - loss: 0.5544 - accuracy: 0.7000\n",
      "Epoch: 132\n",
      "530/530 [==============================] - 2.83s - loss: 0.6002 - accuracy: 0.6906\n",
      "Epoch: 133\n",
      "530/530 [==============================] - 2.88s - loss: 0.5517 - accuracy: 0.7208\n",
      "Epoch: 134\n",
      "530/530 [==============================] - 3.18s - loss: 0.5733 - accuracy: 0.6962\n",
      "Epoch: 135\n",
      "530/530 [==============================] - 3.26s - loss: 0.5445 - accuracy: 0.7075\n",
      "Epoch: 136\n",
      "530/530 [==============================] - 2.98s - loss: 0.5400 - accuracy: 0.7245\n",
      "Epoch: 137\n",
      "530/530 [==============================] - 2.89s - loss: 0.5603 - accuracy: 0.6962\n",
      "Epoch: 138\n",
      "530/530 [==============================] - 2.90s - loss: 0.5459 - accuracy: 0.7113\n",
      "Epoch: 139\n",
      "530/530 [==============================] - 2.94s - loss: 0.5598 - accuracy: 0.6981\n",
      "Epoch: 140\n",
      "530/530 [==============================] - 2.87s - loss: 0.5519 - accuracy: 0.7075\n",
      "Epoch: 141\n",
      "530/530 [==============================] - 3.17s - loss: 0.5643 - accuracy: 0.6566\n",
      "Epoch: 142\n",
      "530/530 [==============================] - 3.03s - loss: 0.5612 - accuracy: 0.7151\n",
      "Epoch: 143\n",
      "530/530 [==============================] - 3.16s - loss: 0.5366 - accuracy: 0.7245\n",
      "Epoch: 144\n",
      "530/530 [==============================] - 2.99s - loss: 0.5664 - accuracy: 0.7000\n",
      "Epoch: 145\n",
      "530/530 [==============================] - 2.97s - loss: 0.5357 - accuracy: 0.6943\n",
      "Epoch: 146\n",
      "530/530 [==============================] - 2.99s - loss: 0.5949 - accuracy: 0.6698\n",
      "Epoch: 147\n",
      "530/530 [==============================] - 2.91s - loss: 0.5332 - accuracy: 0.7264\n",
      "Epoch: 148\n",
      "530/530 [==============================] - 2.93s - loss: 0.5622 - accuracy: 0.7000\n",
      "Epoch: 149\n",
      "530/530 [==============================] - 2.90s - loss: 0.5334 - accuracy: 0.7208\n",
      "Epoch: 150\n",
      "530/530 [==============================] - 2.92s - loss: 0.5812 - accuracy: 0.6830\n",
      "Epoch: 151\n",
      "530/530 [==============================] - 2.86s - loss: 0.5422 - accuracy: 0.7245\n",
      "Epoch: 152\n",
      "530/530 [==============================] - 2.94s - loss: 0.5677 - accuracy: 0.6830\n",
      "Epoch: 153\n",
      "530/530 [==============================] - 2.93s - loss: 0.5623 - accuracy: 0.6830\n",
      "Epoch: 154\n",
      "530/530 [==============================] - 3.08s - loss: 0.5614 - accuracy: 0.6962\n",
      "Epoch: 155\n",
      "530/530 [==============================] - 2.93s - loss: 0.5477 - accuracy: 0.6925\n",
      "Epoch: 156\n",
      "530/530 [==============================] - 2.99s - loss: 0.5636 - accuracy: 0.6887\n",
      "Epoch: 157\n",
      "530/530 [==============================] - 3.09s - loss: 0.5277 - accuracy: 0.7453\n",
      "Epoch: 158\n",
      "530/530 [==============================] - 2.98s - loss: 0.5768 - accuracy: 0.6528\n",
      "Epoch: 159\n",
      "530/530 [==============================] - 2.89s - loss: 0.5719 - accuracy: 0.6830\n",
      "Epoch: 160\n",
      "530/530 [==============================] - 2.83s - loss: 0.5737 - accuracy: 0.6830\n",
      "Epoch: 161\n",
      "530/530 [==============================] - 3.22s - loss: 0.5604 - accuracy: 0.6736\n",
      "Epoch: 162\n",
      "530/530 [==============================] - 3.07s - loss: 0.5886 - accuracy: 0.6736\n",
      "Epoch: 163\n",
      "530/530 [==============================] - 3.12s - loss: 0.5518 - accuracy: 0.6981\n",
      "Epoch: 164\n",
      "530/530 [==============================] - 3.09s - loss: 0.5489 - accuracy: 0.7075\n",
      "Epoch: 165\n",
      "530/530 [==============================] - 2.98s - loss: 0.5632 - accuracy: 0.6925\n",
      "Epoch: 166\n",
      "530/530 [==============================] - 2.92s - loss: 0.5361 - accuracy: 0.7189\n",
      "Epoch: 167\n",
      "530/530 [==============================] - 2.85s - loss: 0.5528 - accuracy: 0.6887\n",
      "Epoch: 168\n",
      "530/530 [==============================] - 2.96s - loss: 0.5310 - accuracy: 0.6981\n",
      "Epoch: 169\n",
      "530/530 [==============================] - 2.90s - loss: 0.5089 - accuracy: 0.7434\n",
      "Epoch: 170\n",
      "530/530 [==============================] - 2.86s - loss: 0.5340 - accuracy: 0.7019\n",
      "Epoch: 171\n",
      "530/530 [==============================] - 2.98s - loss: 0.5395 - accuracy: 0.7170\n",
      "Epoch: 172\n",
      "530/530 [==============================] - 2.98s - loss: 0.5541 - accuracy: 0.7019\n",
      "Epoch: 173\n",
      "530/530 [==============================] - 2.98s - loss: 0.5265 - accuracy: 0.7226\n",
      "Epoch: 174\n",
      "530/530 [==============================] - 2.97s - loss: 0.5461 - accuracy: 0.7208\n",
      "Epoch: 175\n",
      "530/530 [==============================] - 2.95s - loss: 0.5727 - accuracy: 0.6811\n",
      "Epoch: 176\n",
      "530/530 [==============================] - 2.97s - loss: 0.5514 - accuracy: 0.6849\n",
      "Epoch: 177\n",
      "530/530 [==============================] - 2.94s - loss: 0.5452 - accuracy: 0.6830\n",
      "Epoch: 178\n",
      "530/530 [==============================] - 2.94s - loss: 0.5581 - accuracy: 0.6679\n",
      "Epoch: 179\n",
      "530/530 [==============================] - 3.04s - loss: 0.5086 - accuracy: 0.7472\n",
      "Epoch: 180\n",
      "530/530 [==============================] - 2.96s - loss: 0.5010 - accuracy: 0.7528\n",
      "Epoch: 181\n",
      "530/530 [==============================] - 3.16s - loss: 0.5178 - accuracy: 0.7377\n",
      "Epoch: 182\n",
      "530/530 [==============================] - 3.02s - loss: 0.5323 - accuracy: 0.7472\n",
      "Epoch: 183\n",
      "530/530 [==============================] - 3.26s - loss: 0.5323 - accuracy: 0.7226\n",
      "Epoch: 184\n",
      "530/530 [==============================] - 3.07s - loss: 0.5397 - accuracy: 0.7132\n",
      "Epoch: 185\n",
      "530/530 [==============================] - 2.95s - loss: 0.5417 - accuracy: 0.7321\n",
      "Epoch: 186\n",
      "530/530 [==============================] - 3.01s - loss: 0.5183 - accuracy: 0.7264\n",
      "Epoch: 187\n",
      "530/530 [==============================] - 3.03s - loss: 0.5638 - accuracy: 0.6830\n",
      "Epoch: 188\n",
      "530/530 [==============================] - 2.92s - loss: 0.5303 - accuracy: 0.7113\n",
      "Epoch: 189\n",
      "530/530 [==============================] - 2.95s - loss: 0.5197 - accuracy: 0.7226\n",
      "Epoch: 190\n",
      "530/530 [==============================] - 2.91s - loss: 0.5392 - accuracy: 0.7264\n",
      "Epoch: 191\n",
      "530/530 [==============================] - 2.98s - loss: 0.5242 - accuracy: 0.7377\n",
      "Epoch: 192\n",
      "530/530 [==============================] - 3.03s - loss: 0.5395 - accuracy: 0.7057\n",
      "Epoch: 193\n",
      "530/530 [==============================] - 2.99s - loss: 0.5339 - accuracy: 0.7264\n",
      "Epoch: 194\n",
      "530/530 [==============================] - 2.95s - loss: 0.5694 - accuracy: 0.6906\n",
      "Epoch: 195\n",
      "530/530 [==============================] - 2.90s - loss: 0.5247 - accuracy: 0.7094\n",
      "Epoch: 196\n",
      "530/530 [==============================] - 2.97s - loss: 0.5256 - accuracy: 0.7094\n",
      "Epoch: 197\n",
      "530/530 [==============================] - 3.04s - loss: 0.5301 - accuracy: 0.7302\n",
      "Epoch: 198\n",
      "530/530 [==============================] - 2.93s - loss: 0.5505 - accuracy: 0.6925\n",
      "Epoch: 199\n",
      "530/530 [==============================] - 2.96s - loss: 0.5389 - accuracy: 0.7321\n",
      "Epoch: 200\n",
      "530/530 [==============================] - 2.94s - loss: 0.5346 - accuracy: 0.7151\n",
      "Epoch: 201\n",
      "530/530 [==============================] - 3.22s - loss: 0.5394 - accuracy: 0.7151\n",
      "Epoch: 202\n",
      "530/530 [==============================] - 3.11s - loss: 0.5422 - accuracy: 0.7302\n",
      "Epoch: 203\n",
      "530/530 [==============================] - 3.17s - loss: 0.5198 - accuracy: 0.7321\n",
      "Epoch: 204\n",
      "530/530 [==============================] - 2.99s - loss: 0.5043 - accuracy: 0.7340\n",
      "Epoch: 205\n",
      "530/530 [==============================] - 3.01s - loss: 0.5054 - accuracy: 0.7264\n",
      "Epoch: 206\n",
      "530/530 [==============================] - 3.04s - loss: 0.5166 - accuracy: 0.7283\n",
      "Epoch: 207\n",
      "530/530 [==============================] - 2.93s - loss: 0.5347 - accuracy: 0.7340\n",
      "Epoch: 208\n",
      "530/530 [==============================] - 2.91s - loss: 0.5060 - accuracy: 0.7566\n",
      "Epoch: 209\n",
      "530/530 [==============================] - 2.94s - loss: 0.5203 - accuracy: 0.7396\n",
      "Epoch: 210\n",
      "530/530 [==============================] - 2.92s - loss: 0.5120 - accuracy: 0.7434\n",
      "Epoch: 211\n",
      "530/530 [==============================] - 2.90s - loss: 0.5227 - accuracy: 0.7453\n",
      "Epoch: 212\n",
      "530/530 [==============================] - 2.99s - loss: 0.5345 - accuracy: 0.7132\n",
      "Epoch: 213\n",
      "530/530 [==============================] - 2.78s - loss: 0.5094 - accuracy: 0.7491\n",
      "Epoch: 214\n",
      "530/530 [==============================] - 2.90s - loss: 0.5193 - accuracy: 0.7245\n",
      "Epoch: 215\n",
      "530/530 [==============================] - 3.44s - loss: 0.5207 - accuracy: 0.7151\n",
      "Epoch: 216\n",
      "530/530 [==============================] - 3.22s - loss: 0.4891 - accuracy: 0.7509\n",
      "Epoch: 217\n",
      "530/530 [==============================] - 3.04s - loss: 0.4990 - accuracy: 0.7283\n",
      "Epoch: 218\n",
      "530/530 [==============================] - 2.97s - loss: 0.4863 - accuracy: 0.7604\n",
      "Epoch: 219\n",
      "530/530 [==============================] - 3.02s - loss: 0.4999 - accuracy: 0.7547\n",
      "Epoch: 220\n",
      "530/530 [==============================] - 2.91s - loss: 0.5350 - accuracy: 0.7302\n",
      "Epoch: 221\n",
      "530/530 [==============================] - 3.28s - loss: 0.5411 - accuracy: 0.6868\n",
      "Epoch: 222\n",
      "530/530 [==============================] - 2.95s - loss: 0.5140 - accuracy: 0.7170\n",
      "Epoch: 223\n",
      "530/530 [==============================] - 3.19s - loss: 0.5389 - accuracy: 0.7113\n",
      "Epoch: 224\n",
      "530/530 [==============================] - 3.03s - loss: 0.5356 - accuracy: 0.7170\n",
      "Epoch: 225\n",
      "530/530 [==============================] - 3.06s - loss: 0.5198 - accuracy: 0.7340\n",
      "Epoch: 226\n",
      "530/530 [==============================] - 2.96s - loss: 0.5408 - accuracy: 0.7151\n",
      "Epoch: 227\n",
      "530/530 [==============================] - 2.96s - loss: 0.5158 - accuracy: 0.7453\n",
      "Epoch: 228\n",
      "530/530 [==============================] - 2.93s - loss: 0.4987 - accuracy: 0.7774\n",
      "Epoch: 229\n",
      "530/530 [==============================] - 2.93s - loss: 0.5224 - accuracy: 0.7208\n",
      "Epoch: 230\n",
      "530/530 [==============================] - 2.98s - loss: 0.5453 - accuracy: 0.7132\n",
      "Epoch: 231\n",
      "530/530 [==============================] - 3.03s - loss: 0.5504 - accuracy: 0.6906\n",
      "Epoch: 232\n",
      "530/530 [==============================] - 2.95s - loss: 0.5244 - accuracy: 0.6868\n",
      "Epoch: 233\n",
      "530/530 [==============================] - 2.91s - loss: 0.5492 - accuracy: 0.7019\n",
      "Epoch: 234\n",
      "530/530 [==============================] - 2.94s - loss: 0.5186 - accuracy: 0.7377\n",
      "Epoch: 235\n",
      "530/530 [==============================] - 2.98s - loss: 0.4909 - accuracy: 0.7528\n",
      "Epoch: 236\n",
      "530/530 [==============================] - 2.94s - loss: 0.5275 - accuracy: 0.7057\n",
      "Epoch: 237\n",
      "530/530 [==============================] - 2.97s - loss: 0.5088 - accuracy: 0.7377\n",
      "Epoch: 238\n",
      "530/530 [==============================] - 2.95s - loss: 0.5073 - accuracy: 0.7189\n",
      "Epoch: 239\n",
      "530/530 [==============================] - 2.95s - loss: 0.5238 - accuracy: 0.7038\n",
      "Epoch: 240\n",
      "530/530 [==============================] - 3.00s - loss: 0.5125 - accuracy: 0.7264\n",
      "Epoch: 241\n",
      "530/530 [==============================] - 3.30s - loss: 0.5079 - accuracy: 0.7189\n",
      "Epoch: 242\n",
      "530/530 [==============================] - 3.00s - loss: 0.5138 - accuracy: 0.7208\n",
      "Epoch: 243\n",
      "530/530 [==============================] - 3.22s - loss: 0.4896 - accuracy: 0.7321\n",
      "Epoch: 244\n",
      "530/530 [==============================] - 3.06s - loss: 0.4945 - accuracy: 0.7604\n",
      "Epoch: 245\n",
      "530/530 [==============================] - 3.12s - loss: 0.5333 - accuracy: 0.7245\n",
      "Epoch: 246\n",
      "530/530 [==============================] - 2.90s - loss: 0.4822 - accuracy: 0.7642\n",
      "Epoch: 247\n",
      "530/530 [==============================] - 2.89s - loss: 0.5143 - accuracy: 0.7245\n",
      "Epoch: 248\n",
      "530/530 [==============================] - 3.50s - loss: 0.4973 - accuracy: 0.7434\n",
      "Epoch: 249\n",
      "530/530 [==============================] - 2.95s - loss: 0.4936 - accuracy: 0.7226\n",
      "Epoch: 250\n",
      "530/530 [==============================] - 2.93s - loss: 0.4990 - accuracy: 0.7491\n",
      "Epoch: 251\n",
      "530/530 [==============================] - 2.95s - loss: 0.5112 - accuracy: 0.7415\n",
      "Epoch: 252\n",
      "530/530 [==============================] - 3.01s - loss: 0.4967 - accuracy: 0.7604\n",
      "Epoch: 253\n",
      "530/530 [==============================] - 2.94s - loss: 0.5338 - accuracy: 0.7547\n",
      "Epoch: 254\n",
      "530/530 [==============================] - 2.91s - loss: 0.5482 - accuracy: 0.7151\n",
      "Epoch: 255\n",
      "530/530 [==============================] - 2.90s - loss: 0.5014 - accuracy: 0.7509\n",
      "Epoch: 256\n",
      "530/530 [==============================] - 2.97s - loss: 0.5051 - accuracy: 0.7132\n",
      "Epoch: 257\n",
      "530/530 [==============================] - 2.92s - loss: 0.4781 - accuracy: 0.7660\n",
      "Epoch: 258\n",
      "530/530 [==============================] - 2.92s - loss: 0.5001 - accuracy: 0.7509\n",
      "Epoch: 259\n",
      "530/530 [==============================] - 2.94s - loss: 0.4915 - accuracy: 0.7604\n",
      "Epoch: 260\n",
      "530/530 [==============================] - 3.00s - loss: 0.5288 - accuracy: 0.7396\n",
      "Epoch: 261\n",
      "530/530 [==============================] - 3.17s - loss: 0.5316 - accuracy: 0.7170\n",
      "Epoch: 262\n",
      "530/530 [==============================] - 3.12s - loss: 0.5036 - accuracy: 0.7264\n",
      "Epoch: 263\n",
      "530/530 [==============================] - 3.00s - loss: 0.5068 - accuracy: 0.7434\n",
      "Epoch: 264\n",
      "530/530 [==============================] - 2.93s - loss: 0.5111 - accuracy: 0.7170\n",
      "Epoch: 265\n",
      "530/530 [==============================] - 2.99s - loss: 0.4922 - accuracy: 0.7623\n",
      "Epoch: 266\n",
      "530/530 [==============================] - 2.96s - loss: 0.5375 - accuracy: 0.7245\n",
      "Epoch: 267\n",
      "530/530 [==============================] - 3.01s - loss: 0.4847 - accuracy: 0.7566\n",
      "Epoch: 268\n",
      "530/530 [==============================] - 2.96s - loss: 0.4715 - accuracy: 0.7585\n",
      "Epoch: 269\n",
      "530/530 [==============================] - 2.95s - loss: 0.5047 - accuracy: 0.7453\n",
      "Epoch: 270\n",
      "530/530 [==============================] - 2.88s - loss: 0.4704 - accuracy: 0.7547\n",
      "Epoch: 271\n",
      "530/530 [==============================] - 2.92s - loss: 0.5132 - accuracy: 0.7415\n",
      "Epoch: 272\n",
      "530/530 [==============================] - 2.86s - loss: 0.4955 - accuracy: 0.7453\n",
      "Epoch: 273\n",
      "530/530 [==============================] - 2.91s - loss: 0.4927 - accuracy: 0.7642\n",
      "Epoch: 274\n",
      "530/530 [==============================] - 2.93s - loss: 0.5180 - accuracy: 0.7226\n",
      "Epoch: 275\n",
      "530/530 [==============================] - 2.96s - loss: 0.5380 - accuracy: 0.7189\n",
      "Epoch: 276\n",
      "530/530 [==============================] - 2.95s - loss: 0.4968 - accuracy: 0.7472\n",
      "Epoch: 277\n",
      "530/530 [==============================] - 2.96s - loss: 0.5137 - accuracy: 0.7264\n",
      "Epoch: 278\n",
      "530/530 [==============================] - 3.01s - loss: 0.4954 - accuracy: 0.7623\n",
      "Epoch: 279\n",
      "530/530 [==============================] - 2.99s - loss: 0.4967 - accuracy: 0.7415\n",
      "Epoch: 280\n",
      "530/530 [==============================] - 2.92s - loss: 0.4801 - accuracy: 0.7547\n",
      "Epoch: 281\n",
      "530/530 [==============================] - 3.09s - loss: 0.4900 - accuracy: 0.7377\n",
      "Epoch: 282\n",
      "530/530 [==============================] - 2.93s - loss: 0.4790 - accuracy: 0.7679\n",
      "Epoch: 283\n",
      "530/530 [==============================] - 3.14s - loss: 0.5143 - accuracy: 0.7321\n",
      "Epoch: 284\n",
      "530/530 [==============================] - 2.97s - loss: 0.4883 - accuracy: 0.7528\n",
      "Epoch: 285\n",
      "530/530 [==============================] - 3.03s - loss: 0.4799 - accuracy: 0.7566\n",
      "Epoch: 286\n",
      "530/530 [==============================] - 2.96s - loss: 0.4930 - accuracy: 0.7585\n",
      "Epoch: 287\n",
      "530/530 [==============================] - 2.93s - loss: 0.4969 - accuracy: 0.7321\n",
      "Epoch: 288\n",
      "530/530 [==============================] - 2.91s - loss: 0.5049 - accuracy: 0.7208\n",
      "Epoch: 289\n",
      "530/530 [==============================] - 3.06s - loss: 0.4930 - accuracy: 0.7472\n",
      "Epoch: 290\n",
      "530/530 [==============================] - 2.87s - loss: 0.4956 - accuracy: 0.7566\n",
      "Epoch: 291\n",
      "530/530 [==============================] - 2.91s - loss: 0.5139 - accuracy: 0.7434\n",
      "Epoch: 292\n",
      "530/530 [==============================] - 2.93s - loss: 0.5391 - accuracy: 0.7038\n",
      "Epoch: 293\n",
      "530/530 [==============================] - 2.98s - loss: 0.4960 - accuracy: 0.7283\n",
      "Epoch: 294\n",
      "530/530 [==============================] - 2.90s - loss: 0.4687 - accuracy: 0.7453\n",
      "Epoch: 295\n",
      "530/530 [==============================] - 2.92s - loss: 0.4891 - accuracy: 0.7208\n",
      "Epoch: 296\n",
      "530/530 [==============================] - 3.76s - loss: 0.4901 - accuracy: 0.7226\n",
      "Epoch: 297\n",
      "530/530 [==============================] - 2.94s - loss: 0.5030 - accuracy: 0.7377\n",
      "Epoch: 298\n",
      "530/530 [==============================] - 2.89s - loss: 0.4971 - accuracy: 0.7340\n",
      "Epoch: 299\n",
      "530/530 [==============================] - 2.93s - loss: 0.5332 - accuracy: 0.7170\n",
      "Epoch: 300\n",
      "530/530 [==============================] - 2.99s - loss: 0.4997 - accuracy: 0.7245\n",
      "Epoch: 301\n",
      "530/530 [==============================] - 3.18s - loss: 0.4752 - accuracy: 0.7434\n",
      "Epoch: 302\n",
      "530/530 [==============================] - 3.21s - loss: 0.4835 - accuracy: 0.7453\n",
      "Epoch: 303\n",
      "530/530 [==============================] - 3.14s - loss: 0.4614 - accuracy: 0.7660\n",
      "Epoch: 304\n",
      "530/530 [==============================] - 3.08s - loss: 0.4854 - accuracy: 0.7604\n",
      "Epoch: 305\n",
      "530/530 [==============================] - 2.97s - loss: 0.5420 - accuracy: 0.7302\n",
      "Epoch: 306\n",
      "530/530 [==============================] - 3.02s - loss: 0.4923 - accuracy: 0.7604\n",
      "Epoch: 307\n",
      "530/530 [==============================] - 2.97s - loss: 0.4753 - accuracy: 0.7509\n",
      "Epoch: 308\n",
      "530/530 [==============================] - 2.97s - loss: 0.4782 - accuracy: 0.7434\n",
      "Epoch: 309\n",
      "530/530 [==============================] - 2.95s - loss: 0.4957 - accuracy: 0.7528\n",
      "Epoch: 310\n",
      "530/530 [==============================] - 2.98s - loss: 0.4757 - accuracy: 0.7547\n",
      "Epoch: 311\n",
      "530/530 [==============================] - 3.09s - loss: 0.4859 - accuracy: 0.7566\n",
      "Epoch: 312\n",
      "530/530 [==============================] - 3.01s - loss: 0.4673 - accuracy: 0.7528\n",
      "Epoch: 313\n",
      "530/530 [==============================] - 3.01s - loss: 0.4725 - accuracy: 0.7830\n",
      "Epoch: 314\n",
      "530/530 [==============================] - 2.96s - loss: 0.5001 - accuracy: 0.7547\n",
      "Epoch: 315\n",
      "530/530 [==============================] - 3.06s - loss: 0.5032 - accuracy: 0.7415\n",
      "Epoch: 316\n",
      "530/530 [==============================] - 2.94s - loss: 0.4807 - accuracy: 0.7547\n",
      "Epoch: 317\n",
      "530/530 [==============================] - 3.08s - loss: 0.4745 - accuracy: 0.7679\n",
      "Epoch: 318\n",
      "530/530 [==============================] - 2.96s - loss: 0.4676 - accuracy: 0.7660\n",
      "Epoch: 319\n",
      "530/530 [==============================] - 2.99s - loss: 0.4580 - accuracy: 0.7679\n",
      "Epoch: 320\n",
      "530/530 [==============================] - 3.03s - loss: 0.5197 - accuracy: 0.7283\n",
      "Epoch: 321\n",
      "530/530 [==============================] - 3.25s - loss: 0.5206 - accuracy: 0.7151\n",
      "Epoch: 322\n",
      "530/530 [==============================] - 3.12s - loss: 0.5066 - accuracy: 0.7358\n",
      "Epoch: 323\n",
      "530/530 [==============================] - 2.95s - loss: 0.5202 - accuracy: 0.7321\n",
      "Epoch: 324\n",
      "530/530 [==============================] - 2.99s - loss: 0.4550 - accuracy: 0.7679\n",
      "Epoch: 325\n",
      "530/530 [==============================] - 2.97s - loss: 0.4713 - accuracy: 0.7509\n",
      "Epoch: 326\n",
      "530/530 [==============================] - 2.98s - loss: 0.4821 - accuracy: 0.7434\n",
      "Epoch: 327\n",
      "530/530 [==============================] - 2.87s - loss: 0.4581 - accuracy: 0.7736\n",
      "Epoch: 328\n",
      "530/530 [==============================] - 2.94s - loss: 0.4532 - accuracy: 0.7698\n",
      "Epoch: 329\n",
      "530/530 [==============================] - 2.93s - loss: 0.4967 - accuracy: 0.7491\n",
      "Epoch: 330\n",
      "530/530 [==============================] - 2.90s - loss: 0.4973 - accuracy: 0.7358\n",
      "Epoch: 331\n",
      "530/530 [==============================] - 3.11s - loss: 0.4897 - accuracy: 0.7340\n",
      "Epoch: 332\n",
      "530/530 [==============================] - 2.90s - loss: 0.4414 - accuracy: 0.7698\n",
      "Epoch: 333\n",
      "530/530 [==============================] - 2.99s - loss: 0.4695 - accuracy: 0.7472\n",
      "Epoch: 334\n",
      "530/530 [==============================] - 2.96s - loss: 0.4558 - accuracy: 0.7811\n",
      "Epoch: 335\n",
      "530/530 [==============================] - 2.90s - loss: 0.4867 - accuracy: 0.7057\n",
      "Epoch: 336\n",
      "530/530 [==============================] - 2.98s - loss: 0.4763 - accuracy: 0.7472\n",
      "Epoch: 337\n",
      "530/530 [==============================] - 2.99s - loss: 0.5114 - accuracy: 0.7283\n",
      "Epoch: 338\n",
      "530/530 [==============================] - 2.99s - loss: 0.4633 - accuracy: 0.7642\n",
      "Epoch: 339\n",
      "530/530 [==============================] - 3.02s - loss: 0.5204 - accuracy: 0.7415\n",
      "Epoch: 340\n",
      "530/530 [==============================] - 2.99s - loss: 0.4919 - accuracy: 0.7509\n",
      "Epoch: 341\n",
      "530/530 [==============================] - 3.22s - loss: 0.5124 - accuracy: 0.7660\n",
      "Epoch: 342\n",
      "530/530 [==============================] - 3.19s - loss: 0.4813 - accuracy: 0.7642\n",
      "Epoch: 343\n",
      "530/530 [==============================] - 3.04s - loss: 0.4757 - accuracy: 0.7774\n",
      "Epoch: 344\n",
      "530/530 [==============================] - 3.04s - loss: 0.4764 - accuracy: 0.7585\n",
      "Epoch: 345\n",
      "530/530 [==============================] - 2.98s - loss: 0.4613 - accuracy: 0.7660\n",
      "Epoch: 346\n",
      "530/530 [==============================] - 3.04s - loss: 0.4471 - accuracy: 0.7887\n",
      "Epoch: 347\n",
      "530/530 [==============================] - 3.04s - loss: 0.4889 - accuracy: 0.7396\n",
      "Epoch: 348\n",
      "530/530 [==============================] - 2.99s - loss: 0.4972 - accuracy: 0.7377\n",
      "Epoch: 349\n",
      "530/530 [==============================] - 2.96s - loss: 0.4586 - accuracy: 0.7811\n",
      "Epoch: 350\n",
      "530/530 [==============================] - 2.92s - loss: 0.4639 - accuracy: 0.7755\n",
      "Epoch: 351\n",
      "530/530 [==============================] - 2.97s - loss: 0.4681 - accuracy: 0.7660\n",
      "Epoch: 352\n",
      "530/530 [==============================] - 2.86s - loss: 0.4770 - accuracy: 0.7679\n",
      "Epoch: 353\n",
      "530/530 [==============================] - 2.91s - loss: 0.4929 - accuracy: 0.7415\n",
      "Epoch: 354\n",
      "530/530 [==============================] - 2.92s - loss: 0.4993 - accuracy: 0.7340\n",
      "Epoch: 355\n",
      "530/530 [==============================] - 3.00s - loss: 0.4862 - accuracy: 0.7585\n",
      "Epoch: 356\n",
      "530/530 [==============================] - 2.98s - loss: 0.5143 - accuracy: 0.7396\n",
      "Epoch: 357\n",
      "530/530 [==============================] - 2.93s - loss: 0.5123 - accuracy: 0.7472\n",
      "Epoch: 358\n",
      "530/530 [==============================] - 2.89s - loss: 0.4728 - accuracy: 0.7547\n",
      "Epoch: 359\n",
      "530/530 [==============================] - 2.95s - loss: 0.4966 - accuracy: 0.7528\n",
      "Epoch: 360\n",
      "530/530 [==============================] - 3.01s - loss: 0.5186 - accuracy: 0.7264\n",
      "Epoch: 361\n",
      "530/530 [==============================] - 3.16s - loss: 0.4785 - accuracy: 0.7623\n",
      "Epoch: 362\n",
      "530/530 [==============================] - 3.25s - loss: 0.4965 - accuracy: 0.7434\n",
      "Epoch: 363\n",
      "530/530 [==============================] - 3.03s - loss: 0.4827 - accuracy: 0.7623\n",
      "Epoch: 364\n",
      "530/530 [==============================] - 3.02s - loss: 0.5043 - accuracy: 0.7604\n",
      "Epoch: 365\n",
      "530/530 [==============================] - 2.86s - loss: 0.5072 - accuracy: 0.7415\n",
      "Epoch: 366\n",
      "530/530 [==============================] - 2.98s - loss: 0.4735 - accuracy: 0.7642\n",
      "Epoch: 367\n",
      "530/530 [==============================] - 2.94s - loss: 0.5122 - accuracy: 0.7245\n",
      "Epoch: 368\n",
      "530/530 [==============================] - 2.94s - loss: 0.5043 - accuracy: 0.7283\n",
      "Epoch: 369\n",
      "530/530 [==============================] - 2.90s - loss: 0.5128 - accuracy: 0.7321\n",
      "Epoch: 370\n",
      "530/530 [==============================] - 2.94s - loss: 0.5123 - accuracy: 0.7566\n",
      "Epoch: 371\n",
      "530/530 [==============================] - 2.98s - loss: 0.5209 - accuracy: 0.7396\n",
      "Epoch: 372\n",
      "530/530 [==============================] - 2.99s - loss: 0.5122 - accuracy: 0.7472\n",
      "Epoch: 373\n",
      "530/530 [==============================] - 2.95s - loss: 0.5111 - accuracy: 0.7509\n",
      "Epoch: 374\n",
      "530/530 [==============================] - 3.00s - loss: 0.5091 - accuracy: 0.7132\n",
      "Epoch: 375\n",
      "530/530 [==============================] - 3.01s - loss: 0.5379 - accuracy: 0.7264\n",
      "Epoch: 376\n",
      "530/530 [==============================] - 2.95s - loss: 0.5120 - accuracy: 0.7189\n",
      "Epoch: 377\n",
      "530/530 [==============================] - 2.95s - loss: 0.4986 - accuracy: 0.7189\n",
      "Epoch: 378\n",
      "530/530 [==============================] - 2.99s - loss: 0.4845 - accuracy: 0.7547\n",
      "Epoch: 379\n",
      "530/530 [==============================] - 2.96s - loss: 0.4933 - accuracy: 0.7566\n",
      "Epoch: 380\n",
      "530/530 [==============================] - 3.07s - loss: 0.5083 - accuracy: 0.7075\n",
      "Epoch: 381\n",
      "530/530 [==============================] - 3.31s - loss: 0.4792 - accuracy: 0.7453\n",
      "Epoch: 382\n",
      "530/530 [==============================] - 3.03s - loss: 0.5194 - accuracy: 0.7396\n",
      "Epoch: 383\n",
      "530/530 [==============================] - 3.24s - loss: 0.4920 - accuracy: 0.7679\n",
      "Epoch: 384\n",
      "530/530 [==============================] - 3.00s - loss: 0.4999 - accuracy: 0.7453\n",
      "Epoch: 385\n",
      "530/530 [==============================] - 2.91s - loss: 0.5044 - accuracy: 0.7358\n",
      "Epoch: 386\n",
      "530/530 [==============================] - 2.99s - loss: 0.4434 - accuracy: 0.7811\n",
      "Epoch: 387\n",
      "530/530 [==============================] - 2.92s - loss: 0.4693 - accuracy: 0.7774\n",
      "Epoch: 388\n",
      "530/530 [==============================] - 3.07s - loss: 0.4942 - accuracy: 0.7717\n",
      "Epoch: 389\n",
      "530/530 [==============================] - 2.97s - loss: 0.5114 - accuracy: 0.7283\n",
      "Epoch: 390\n",
      "530/530 [==============================] - 2.89s - loss: 0.4531 - accuracy: 0.7792\n",
      "Epoch: 391\n",
      "530/530 [==============================] - 2.90s - loss: 0.5157 - accuracy: 0.7283\n",
      "Epoch: 392\n",
      "530/530 [==============================] - 2.95s - loss: 0.4865 - accuracy: 0.7604\n",
      "Epoch: 393\n",
      "530/530 [==============================] - 2.99s - loss: 0.4971 - accuracy: 0.7566\n",
      "Epoch: 394\n",
      "530/530 [==============================] - 2.89s - loss: 0.5030 - accuracy: 0.7226\n",
      "Epoch: 395\n",
      "530/530 [==============================] - 2.96s - loss: 0.4442 - accuracy: 0.7849\n",
      "Epoch: 396\n",
      "530/530 [==============================] - 3.08s - loss: 0.4909 - accuracy: 0.7396\n",
      "Epoch: 397\n",
      "530/530 [==============================] - 2.98s - loss: 0.4899 - accuracy: 0.7453\n",
      "Epoch: 398\n",
      "530/530 [==============================] - 2.93s - loss: 0.4794 - accuracy: 0.7623\n",
      "Epoch: 399\n",
      "530/530 [==============================] - 3.05s - loss: 0.4781 - accuracy: 0.7509\n",
      "Epoch: 400\n",
      "530/530 [==============================] - 3.00s - loss: 0.4903 - accuracy: 0.7415\n",
      "Epoch: 401\n",
      "530/530 [==============================] - 3.22s - loss: 0.5050 - accuracy: 0.7113\n",
      "Epoch: 402\n",
      "530/530 [==============================] - 2.97s - loss: 0.4536 - accuracy: 0.7811\n",
      "Epoch: 403\n",
      "530/530 [==============================] - 3.31s - loss: 0.4996 - accuracy: 0.7528\n",
      "Epoch: 404\n",
      "530/530 [==============================] - 2.95s - loss: 0.4558 - accuracy: 0.7887\n",
      "Epoch: 405\n",
      "530/530 [==============================] - 3.00s - loss: 0.4533 - accuracy: 0.7736\n",
      "Epoch: 406\n",
      "530/530 [==============================] - 2.94s - loss: 0.4450 - accuracy: 0.7943\n",
      "Epoch: 407\n",
      "530/530 [==============================] - 3.01s - loss: 0.4659 - accuracy: 0.7679\n",
      "Epoch: 408\n",
      "530/530 [==============================] - 2.96s - loss: 0.4464 - accuracy: 0.7868\n",
      "Epoch: 409\n",
      "530/530 [==============================] - 2.93s - loss: 0.4703 - accuracy: 0.7623\n",
      "Epoch: 410\n",
      "530/530 [==============================] - 3.01s - loss: 0.4356 - accuracy: 0.7717\n",
      "Epoch: 411\n",
      "530/530 [==============================] - 2.91s - loss: 0.4751 - accuracy: 0.7736\n",
      "Epoch: 412\n",
      "530/530 [==============================] - 3.00s - loss: 0.4773 - accuracy: 0.7509\n",
      "Epoch: 413\n",
      "530/530 [==============================] - 2.97s - loss: 0.4875 - accuracy: 0.7302\n",
      "Epoch: 414\n",
      "530/530 [==============================] - 2.93s - loss: 0.4763 - accuracy: 0.7792\n",
      "Epoch: 415\n",
      "530/530 [==============================] - 2.90s - loss: 0.4775 - accuracy: 0.7679\n",
      "Epoch: 416\n",
      "530/530 [==============================] - 2.93s - loss: 0.4405 - accuracy: 0.7849\n",
      "Epoch: 417\n",
      "530/530 [==============================] - 2.96s - loss: 0.4509 - accuracy: 0.7830\n",
      "Epoch: 418\n",
      "530/530 [==============================] - 2.93s - loss: 0.4543 - accuracy: 0.7830\n",
      "Epoch: 419\n",
      "530/530 [==============================] - 2.90s - loss: 0.4494 - accuracy: 0.7774\n",
      "Epoch: 420\n",
      "530/530 [==============================] - 2.91s - loss: 0.4355 - accuracy: 0.7830\n",
      "Epoch: 421\n",
      "530/530 [==============================] - 3.14s - loss: 0.4492 - accuracy: 0.7755\n",
      "Epoch: 422\n",
      "530/530 [==============================] - 2.99s - loss: 0.4384 - accuracy: 0.7868\n",
      "Epoch: 423\n",
      "530/530 [==============================] - 3.20s - loss: 0.4561 - accuracy: 0.7868\n",
      "Epoch: 424\n",
      "530/530 [==============================] - 2.89s - loss: 0.4692 - accuracy: 0.7774\n",
      "Epoch: 425\n",
      "530/530 [==============================] - 3.04s - loss: 0.4974 - accuracy: 0.7283\n",
      "Epoch: 426\n",
      "530/530 [==============================] - 2.89s - loss: 0.4929 - accuracy: 0.7453\n",
      "Epoch: 427\n",
      "530/530 [==============================] - 2.96s - loss: 0.4599 - accuracy: 0.7283\n",
      "Epoch: 428\n",
      "530/530 [==============================] - 2.93s - loss: 0.4688 - accuracy: 0.7566\n",
      "Epoch: 429\n",
      "530/530 [==============================] - 3.00s - loss: 0.4506 - accuracy: 0.7679\n",
      "Epoch: 430\n",
      "530/530 [==============================] - 2.92s - loss: 0.4388 - accuracy: 0.7925\n",
      "Epoch: 431\n",
      "530/530 [==============================] - 2.86s - loss: 0.4352 - accuracy: 0.8019\n",
      "Model saved based on threshold accuracy...\n",
      "Epoch: 432\n",
      "530/530 [==============================] - 3.03s - loss: 0.4382 - accuracy: 0.7925\n",
      "Epoch: 433\n",
      "530/530 [==============================] - 2.95s - loss: 0.4396 - accuracy: 0.7962\n",
      "Epoch: 434\n",
      "530/530 [==============================] - 2.95s - loss: 0.4600 - accuracy: 0.7887\n",
      "Epoch: 435\n",
      "530/530 [==============================] - 3.03s - loss: 0.4854 - accuracy: 0.7434\n",
      "Epoch: 436\n",
      "530/530 [==============================] - 3.02s - loss: 0.5035 - accuracy: 0.7264\n",
      "Epoch: 437\n",
      "530/530 [==============================] - 2.93s - loss: 0.4583 - accuracy: 0.7698\n",
      "Epoch: 438\n",
      "530/530 [==============================] - 2.95s - loss: 0.4690 - accuracy: 0.7585\n",
      "Epoch: 439\n",
      "530/530 [==============================] - 3.00s - loss: 0.4152 - accuracy: 0.7962\n",
      "Model saved based on threshold loss...\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "count = 1\n",
    "x = np.array(train_labels).shape[0]\n",
    "print('Train on {} samples...' .format(x))\n",
    "while True:\n",
    "    t1 = time.time()\n",
    "    history = rnn.fit(np.array(result), np.array(train_labels), epochs = 1, batch_size = batch_size, shuffle = False, verbose = 0)\n",
    "    print('Epoch: {}' .format(count))\n",
    "    print('{}/{} [==============================] - {}s - loss: {} - accuracy: {}' .format(x, x, '%.2f'%(time.time() - t1), '%.4f'%(history.history['loss'][-1]), '%.4f'%(history.history['accuracy'][-1])))\n",
    "    count += 1\n",
    "    \n",
    "    if(history.history['accuracy'][-1] >= 0.8):\n",
    "        print('Model saved based on threshold accuracy...')\n",
    "        rnn.save('deepfake_predictor_acc_0.8.h5')\n",
    "        early_stop_loss(count)\n",
    "        break\n",
    "        \n",
    "    if(history.history['loss'][-1] <= 0.43):\n",
    "        print('Model saved based on threshold loss...')\n",
    "        rnn.save('deepfake_predictor_loss_0.43.h5')\n",
    "        early_stop_acc(count)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting existing model from memory\n",
    "\n",
    "del rnn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
